{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95606f4c",
   "metadata": {},
   "source": [
    "## General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39461d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "file_path = r\"C:\\Users\\simonk03\\Downloads\\Mid Project\\Final\\Provision2024.xlsx\"  \n",
    "if os.path.exists(file_path):  # Check if the file exists\n",
    "    excel_data = pd.ExcelFile(file_path)  # Load the Excel file\n",
    "    actual_potential = excel_data.parse(\"Actual&Potential\")  # Load 'Actual&Potential' sheet\n",
    "    loss_tree = excel_data.parse(\"Loss Tree\")  # Load 'Loss Tree' sheet\n",
    "    print(\"File loaded successfully!\")  # Confirm successful loading\n",
    "else:\n",
    "    print(f\"File not found at {file_path}. Please check the path and try again.\")  #if file is not found|\n",
    "    \n",
    "    \n",
    "    import os\n",
    "\n",
    "# Define your full file path\n",
    "csv_file_path = r'C:\\Users\\simonk03\\OneDrive - Heineken International\\Data siense\\Master Data sciense\\PowerBI Projects\\Mid project\\Finaalsss\\Mid project Scrapping\\cleaned_scrapping_provision_from_modified1.csv'\n",
    "\n",
    "# Ensure the folder exists\n",
    "os.makedirs(os.path.dirname(csv_file_path), exist_ok=True)\n",
    "\n",
    "# Now save the CSV\n",
    "actual_potential.to_csv(csv_file_path, index=False)\n",
    "print(f\"Data successfully saved to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d877e01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_potential.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d955a2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_tree.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195a5161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Handle Missing Values in \"Actual&Potential\"\n",
    "actual_potential['Product'].fillna('Unknown', inplace=True) # Fill missing 'Product' with 'Unknown'\n",
    "actual_potential['Risk Level'].fillna('Unknown', inplace=True) # Fill missing 'Risk Level' with 'Unknown'\n",
    "actual_potential['Quantity'] = pd.to_numeric(actual_potential['Quantity'], errors='coerce') # Convert 'Quantity' to numeric\n",
    "actual_potential['Quantity'].fillna(actual_potential['Quantity'].mean(), inplace=True) # Fill NaN 'Quantity' with the mean value\n",
    "print(\"Step 1: Missing values handled in 'Actual&Potential'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39819a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Handle Missing Values in \"Loss Tree\"\n",
    "\n",
    "# Check if the 'Category' column exists in the DataFrame\n",
    "if 'Category' in loss_tree.columns:\n",
    "    # Convert 'Category' to string type and fill missing values with 'Unknown'\n",
    "    loss_tree['Category'] = loss_tree['Category'].astype(str)\n",
    "    loss_tree['Category'].fillna('Unknown', inplace=True)\n",
    "else:\n",
    "    # Log a message if 'Category' column is not found\n",
    "    print(\"'Category' column not found in 'Loss Tree'.\")\n",
    "\n",
    "# Check if the 'Sub-Category' column exists in the DataFrame\n",
    "if 'Sub-Category' in loss_tree.columns:\n",
    "    # Convert 'Sub-Category' to string type and fill missing values with 'Unknown'\n",
    "    loss_tree['Sub-Category'] = loss_tree['Sub-Category'].astype(str)\n",
    "    loss_tree['Sub-Category'].fillna('Unknown', inplace=True)\n",
    "else:\n",
    "    # Log a message if 'Sub-Category' column is not found\n",
    "    print(\"'Sub-Category' column not found in 'Loss Tree'.\")\n",
    "\n",
    "# Log the completion of missing value handling\n",
    "print(\"Missing values handled in 'Loss Tree'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e585778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Handle Duplicates in \"Actual&Potential\"\n",
    "# Removing rows that have the same 'Product', 'Risk Level', and 'Quantity'\n",
    "# We'll keep the first occurrence and drop others\n",
    "actual_potential.drop_duplicates(subset=['Product', 'Risk Level', 'Quantity'], keep='first', inplace=True)\n",
    "print(\"Duplicates removed in 'Actual&Potential'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305f8a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Handle Inconsistent Data Types in \"Actual&Potential\"\n",
    "# Convert 'Quantity' column to numeric, invalid values will be set as NaN\n",
    "actual_potential['Quantity'] = pd.to_numeric(actual_potential['Quantity'], errors='coerce')\n",
    "# Convert 'Product' to string type (even if it's already a string, this ensures consistency)\n",
    "actual_potential['Product'] = actual_potential['Product'].astype(str)\n",
    "print(\"Data types fixed in 'Actual&Potential'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfb8db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Handle Misplaced Rows/Irrelevant Data in \"Actual&Potential\"\n",
    "# Remove rows where 'Segment' is '---' and 'Plant' is '###'\n",
    "actual_potential = actual_potential[actual_potential['Segment'] != '---']\n",
    "actual_potential = actual_potential[actual_potential['Plant'] != '###']\n",
    "print(\"Misplaced rows removed in 'Actual&Potential'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7571a959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Handle Outliers in \"Actual&Potential\"\n",
    "# Replace extreme values (above 1000) with NaN and then fill with the column mean\n",
    "actual_potential['Quantity'] = actual_potential['Quantity'].apply(lambda x: x if x < 1000 else np.nan)\n",
    "actual_potential['Quantity'].fillna(actual_potential['Quantity'].mean(), inplace=True)\n",
    "print(\"Outliers handled in 'Actual&Potential'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c022697c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Handle Incorrect Date Formats in \"Actual&Potential\"\n",
    "# Convert 'Date' column to datetime, replacing invalid formats with NaT\n",
    "actual_potential['Date'] = pd.to_datetime(actual_potential['Date'], errors='coerce')\n",
    "# Fill missing date values with a default date\n",
    "actual_potential['Date'].fillna('2024-01-01', inplace=True)\n",
    "print(\"Date formats corrected in 'Actual&Potential'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af81452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Handle Invalid Column Headers in \"Actual&Potential\"\n",
    "# Remove duplicated column names\n",
    "actual_potential = actual_potential.loc[:, ~actual_potential.columns.duplicated()]\n",
    "print(\"Invalid column headers resolved in 'Actual&Potential'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828d7512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Handle Loss Tree Specific Errors\n",
    "# Handle Missing Values in \"Loss Tree\"\n",
    "loss_tree['Category'] = loss_tree.get('Category', pd.Series('Unknown', index=loss_tree.index)).fillna('Unknown')\n",
    "loss_tree['Sub-Category'] = loss_tree.get('Sub-Category', pd.Series('Unknown', index=loss_tree.index)).fillna('Unknown')\n",
    "print(\"Missing values handled in 'Loss Tree'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc5f302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Save the cleaned data to a new Excel file\n",
    "output_path = r'C:\\Users\\simonk03\\Documents\\cleaned_scrapping_provision_from_modified1.xlsx'  # Changed path\n",
    "try:\n",
    "    with pd.ExcelWriter(output_path) as writer:\n",
    "        actual_potential.to_excel(writer, index=False, sheet_name=\"Actual&Potential\")\n",
    "        loss_tree.to_excel(writer, index=False, sheet_name=\"Loss Tree\")\n",
    "    print(f\"Data cleaned and saved to '{output_path}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving the file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ab77d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Scrapping Month' to numeric, remove out-of-bounds values, and convert to datetime\n",
    "actual_potential['Scrapping Month'] = pd.to_datetime(\n",
    "    pd.to_numeric(actual_potential['Scrapping Month'], errors='coerce').clip(lower=1), \n",
    "    origin='unix', unit='D', errors='coerce').dt.strftime('%B %Y')\n",
    "\n",
    "print(\"Scrapping Month format changed to 'Month Year'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e683425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Save the cleaned data to a new Excel file\n",
    "output_path = r'C:\\Users\\simonk03\\Documents\\cleaned_scrapping_provision_from_modified1.xlsx'  # Changed path\n",
    "try:\n",
    "    with pd.ExcelWriter(output_path) as writer:\n",
    "        actual_potential.to_excel(writer, index=False, sheet_name=\"Actual&Potential\")\n",
    "        loss_tree.to_excel(writer, index=False, sheet_name=\"Loss Tree\")\n",
    "    print(f\"Data cleaned and saved to '{output_path}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving the file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28cd4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data as a CSV file\n",
    "csv_file_path = r'C:\\Users\\simonk03\\Downloads\\Final Project 26 June\\cleaned_scrapping_provision_from_modified1.csv'\n",
    "actual_potential.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(f\"✅ Data successfully saved to: {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f80221",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c41e592e",
   "metadata": {},
   "source": [
    "## Scrap Analysis & Cleaning Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0013c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the destination CSV path\n",
    "csv_file_path = r'C:\\Users\\simonk03\\Downloads\\Final Project 26 June\\cleaned_scrapping_provision_from_modified1.csv'\n",
    "\n",
    "# Ensure the folder exists\n",
    "os.makedirs(os.path.dirname(csv_file_path), exist_ok=True)\n",
    "\n",
    "# Save the DataFrame\n",
    "actual_potential.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(f\"✅ Data successfully saved to: {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79a53d9",
   "metadata": {},
   "source": [
    "## Scrap Analysis & Machine Learning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39688c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ✅ Correct path (only one valid absolute path, no nesting)\n",
    "file_path = r'C:\\Users\\simonk03\\Downloads\\Final Project 26 June\\26 June last\\cleaned_scrapping_provision_from_modified1.csv'\n",
    "\n",
    "# Load the cleaned dataset\n",
    "data = pd.read_csv(file_path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32726009",
   "metadata": {},
   "source": [
    "## Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc23864",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "sns.histplot(data['Quantity'], kde=True, bins=30, color='teal')\n",
    "plt.title('Distribution of Quantity')\n",
    "plt.xlabel('Quantity')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(data=data, x='Risk Level', palette='Set2')\n",
    "plt.title('Risk Level Count')\n",
    "plt.xlabel('Risk Level')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.boxplot(x='Value in EGP', data=data, color='lightblue')\n",
    "plt.title('Boxplot of Value in EGP')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09444028",
   "metadata": {},
   "source": [
    "## Bivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c78f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.scatterplot(data=data, x='Quantity', y='Value in EGP', hue='Risk Level', palette='Set1')\n",
    "plt.title('Value vs. Quantity by Risk Level')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(data.corr(numeric_only=True), annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734301eb",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99717f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode relevant categorical columns including Product\n",
    "categorical_features = ['Plant', 'Segement', 'Classification', 'Product']\n",
    "data_encoded = pd.get_dummies(data[categorical_features], drop_first=True)\n",
    "\n",
    "# Example of numeric columns if needed\n",
    "numeric_columns = ['Value in EGP'] if 'Value in EGP' in data.columns else []\n",
    "\n",
    "# Combine encoded categorical and numeric features\n",
    "features = pd.concat([data_encoded, data[numeric_columns]], axis=1)\n",
    "\n",
    "# Define the target variable\n",
    "target = data['Risk Level']\n",
    "\n",
    "# Split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdecaaa",
   "metadata": {},
   "source": [
    "## Modeling & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc606f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode relevant categorical columns including Product\n",
    "categorical_features = ['Plant', 'Segement', 'Classification', 'Product']\n",
    "data_encoded = pd.get_dummies(data[categorical_features], drop_first=True)\n",
    "\n",
    "# Example of numeric columns if needed\n",
    "numeric_columns = ['Value in EGP'] if 'Value in EGP' in data.columns else []\n",
    "\n",
    "# Combine encoded categorical and numeric features\n",
    "features = pd.concat([data_encoded, data[numeric_columns]], axis=1)\n",
    "\n",
    "# Define the target variable\n",
    "target = data['Risk Level']\n",
    "\n",
    "# Split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468760ef",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe59aff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Encode target variable\n",
    "target_encoded = data['Risk Level'].astype('category')\n",
    "y = target_encoded.cat.codes\n",
    "\n",
    "# Encode features\n",
    "categorical_features = ['Plant', 'Segement', 'Classification', 'Product']\n",
    "data_encoded = pd.get_dummies(data[categorical_features], drop_first=True)\n",
    "\n",
    "# Add numeric features (if available)\n",
    "if 'Value in EGP' in data.columns:\n",
    "    data_encoded['Value in EGP'] = data['Value in EGP']\n",
    "\n",
    "# Drop rows with NaNs\n",
    "X_clean = data_encoded.dropna()\n",
    "y_clean = y[X_clean.index]\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_clean, y_clean, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Random Forest\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "print(\"✅ Random Forest Results:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(f\"Random Forest Accuracy: {accuracy_score(y_test, y_pred_rf):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e9ba6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rf.feature_importances_\n",
    "feat_names = features.columns\n",
    "feat_imp = pd.Series(importances, index=feat_names).sort_values(ascending=False)[:10]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "feat_imp.plot(kind='barh', color='orange')\n",
    "plt.title('Top 10 Feature Importances - Random Forest')\n",
    "plt.xlabel('Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a64996b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d595bf79",
   "metadata": {},
   "source": [
    "## Second Model: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c03cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Train Logistic Regression\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred_log = logreg.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"✅ Logistic Regression Results:\")\n",
    "print(classification_report(y_test, y_pred_log))\n",
    "print(f\"Logistic Regression Accuracy: {accuracy_score(y_test, y_pred_log):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
